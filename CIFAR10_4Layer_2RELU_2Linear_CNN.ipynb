{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "torch.manual_seed(12)\n",
    "torch.cuda.manual_seed(12)\n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on device=cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# If we are on a CUDA machine, then this should print a CUDA device, but we are not, so it will run on CPU:\n",
    "print(f'Working on device={device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "\n",
    "# each CIFAR image is RGB 32x32, so it is an 3D array [3,32,32]\n",
    "# we will flatten the image as vector dim=3*32*32\n",
    "input_size = 3*32*32\n",
    "\n",
    "# we have 10 classes\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "num_epochs = 50\n",
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset,\n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        \n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "        #self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = ConvNet()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.7759 Test acc: 0.3931 time=0:00:28.687709\n",
      "Epoch [2/50], Loss: 1.5782 Test acc: 0.4183 time=0:00:30.382453\n",
      "Epoch [3/50], Loss: 1.5346 Test acc: 0.4596 time=0:00:29.551568\n",
      "Epoch [4/50], Loss: 1.5090 Test acc: 0.4697 time=0:00:29.328534\n",
      "Epoch [5/50], Loss: 1.4796 Test acc: 0.4432 time=0:00:29.057410\n",
      "Epoch [6/50], Loss: 1.4781 Test acc: 0.4563 time=0:00:29.598949\n",
      "Epoch [7/50], Loss: 1.4636 Test acc: 0.4714 time=0:00:29.765057\n",
      "Epoch [8/50], Loss: 1.4496 Test acc: 0.4426 time=0:00:30.251707\n",
      "Epoch [9/50], Loss: 1.4493 Test acc: 0.4601 time=0:00:29.903571\n",
      "Epoch [10/50], Loss: 1.4262 Test acc: 0.4691 time=0:00:29.858458\n",
      "Epoch [11/50], Loss: 1.4281 Test acc: 0.4839 time=0:00:30.007587\n",
      "Epoch [12/50], Loss: 1.4262 Test acc: 0.4677 time=0:00:30.028156\n",
      "Epoch [13/50], Loss: 1.4232 Test acc: 0.4872 time=0:00:29.919388\n",
      "Epoch [14/50], Loss: 1.4071 Test acc: 0.4898 time=0:00:29.786341\n",
      "Epoch [15/50], Loss: 1.4077 Test acc: 0.4894 time=0:00:30.037568\n",
      "Epoch [16/50], Loss: 1.3872 Test acc: 0.4873 time=0:00:29.751080\n",
      "Epoch [17/50], Loss: 1.4069 Test acc: 0.4824 time=0:00:30.167744\n",
      "Epoch [18/50], Loss: 1.4020 Test acc: 0.4827 time=0:00:29.877461\n",
      "Epoch [19/50], Loss: 1.3987 Test acc: 0.4809 time=0:00:29.810680\n",
      "Epoch [20/50], Loss: 1.3884 Test acc: 0.4777 time=0:00:29.865915\n",
      "Epoch [21/50], Loss: 1.3939 Test acc: 0.4895 time=0:00:29.827584\n",
      "Epoch [22/50], Loss: 1.3790 Test acc: 0.4972 time=0:00:29.843010\n",
      "Epoch [23/50], Loss: 1.3901 Test acc: 0.5005 time=0:00:29.695477\n",
      "Epoch [24/50], Loss: 1.3722 Test acc: 0.4908 time=0:00:29.850679\n",
      "Epoch [25/50], Loss: 1.3815 Test acc: 0.4997 time=0:00:29.678224\n",
      "Epoch [26/50], Loss: 1.3764 Test acc: 0.4869 time=0:00:30.381953\n",
      "Epoch [27/50], Loss: 1.3717 Test acc: 0.4812 time=0:00:29.756049\n",
      "Epoch [28/50], Loss: 1.3720 Test acc: 0.4741 time=0:00:29.932697\n",
      "Epoch [29/50], Loss: 1.3708 Test acc: 0.4975 time=0:00:29.950650\n",
      "Epoch [30/50], Loss: 1.3699 Test acc: 0.4873 time=0:00:29.826411\n",
      "Epoch [31/50], Loss: 1.3741 Test acc: 0.4957 time=0:00:29.873370\n",
      "Epoch [32/50], Loss: 1.3733 Test acc: 0.4416 time=0:00:29.894804\n",
      "Epoch [33/50], Loss: 1.3709 Test acc: 0.5025 time=0:00:29.856453\n",
      "Epoch [34/50], Loss: 1.3715 Test acc: 0.4881 time=0:00:30.012619\n",
      "Epoch [35/50], Loss: 1.3786 Test acc: 0.479 time=0:00:30.259144\n",
      "Epoch [36/50], Loss: 1.3772 Test acc: 0.4902 time=0:00:29.993506\n",
      "Epoch [37/50], Loss: 1.3654 Test acc: 0.4766 time=0:00:30.053988\n",
      "Epoch [38/50], Loss: 1.3655 Test acc: 0.4849 time=0:00:28.867508\n",
      "Epoch [39/50], Loss: 1.3615 Test acc: 0.4879 time=0:00:29.511596\n",
      "Epoch [40/50], Loss: 1.3578 Test acc: 0.4878 time=0:00:28.975814\n",
      "Epoch [41/50], Loss: 1.3640 Test acc: 0.4802 time=0:00:28.662469\n",
      "Epoch [42/50], Loss: 1.3542 Test acc: 0.4867 time=0:00:28.731599\n",
      "Epoch [43/50], Loss: 1.3638 Test acc: 0.4929 time=0:00:28.534095\n",
      "Epoch [44/50], Loss: 1.3549 Test acc: 0.4917 time=0:00:28.818203\n",
      "Epoch [45/50], Loss: 1.3662 Test acc: 0.4906 time=0:00:28.853884\n",
      "Epoch [46/50], Loss: 1.3607 Test acc: 0.5111 time=0:00:28.510313\n",
      "Epoch [47/50], Loss: 1.3629 Test acc: 0.4763 time=0:00:28.666454\n",
      "Epoch [48/50], Loss: 1.3645 Test acc: 0.4835 time=0:00:29.281780\n",
      "Epoch [49/50], Loss: 1.3543 Test acc: 0.4896 time=0:00:28.715684\n",
      "Epoch [50/50], Loss: 1.3613 Test acc: 0.5007 time=0:00:28.729147\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        # move data to device (GPU if enabled, else CPU do nothing)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss = epoch_loss / len(trainloader)\n",
    "\n",
    "    time_elapsed = datetime.now() - start_time\n",
    "\n",
    "    # Test the model\n",
    "\n",
    "    # set our model in the training mode\n",
    "    net.eval()\n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Accuracy of the network on the 10000 test images\n",
    "    acc = correct/total\n",
    "    print(\n",
    "        f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f} Test acc: {acc} time={time_elapsed}')\n",
    "\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed accuracy per class\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "                label = labels[i]\n",
    "                class_correct[label] += c[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of %5s : %2d %%' % (\n",
    "        classes[i], 100 * class_correct[i] / class_total[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myPy3",
   "language": "python",
   "name": "mypy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
